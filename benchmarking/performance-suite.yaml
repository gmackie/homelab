# Performance Benchmarking Suite for Multi-Architecture Homelab
---
apiVersion: v1
kind: Namespace
metadata:
  name: benchmarking
  labels:
    name: benchmarking

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: benchmark-config
  namespace: benchmarking
data:
  benchmark-suite.yaml: |
    # Benchmark test suite configuration
    benchmarks:
      cpu_intensive:
        description: "CPU-intensive workload benchmark"
        duration: "300s"
        metrics: ["cpu_usage", "instructions_per_second", "context_switches"]
        variants:
          - name: "prime_calculation"
            command: "sysbench cpu --threads=4 --cpu-max-prime=20000 run"
          - name: "compression"
            command: "dd if=/dev/zero bs=1M count=100 | gzip -9 > /dev/null"
          - name: "matrix_multiply"
            command: "python3 /benchmarks/matrix_benchmark.py"
      
      memory_intensive:
        description: "Memory bandwidth and latency benchmark"
        duration: "180s"
        metrics: ["memory_bandwidth", "memory_latency", "cache_performance"]
        variants:
          - name: "memory_bandwidth"
            command: "sysbench memory --memory-block-size=1M --memory-total-size=1G run"
          - name: "memory_random"
            command: "sysbench memory --memory-access-mode=rnd run"
      
      storage_performance:
        description: "Storage I/O performance benchmark"
        duration: "240s"
        metrics: ["iops", "throughput", "latency"]
        variants:
          - name: "sequential_read"
            command: "sysbench fileio --file-test-mode=seqrd --file-total-size=1G prepare && sysbench fileio --file-test-mode=seqrd run"
          - name: "random_write"
            command: "sysbench fileio --file-test-mode=rndwr --file-total-size=1G prepare && sysbench fileio --file-test-mode=rndwr run"
      
      network_performance:
        description: "Network throughput and latency benchmark"
        duration: "120s"
        metrics: ["bandwidth", "latency", "packet_loss"]
        variants:
          - name: "tcp_throughput"
            command: "iperf3 -c benchmark-server -t 60"
          - name: "udp_throughput"
            command: "iperf3 -c benchmark-server -u -t 60"
      
      container_performance:
        description: "Container startup and runtime performance"
        duration: "60s"
        metrics: ["startup_time", "memory_overhead", "cpu_overhead"]
        variants:
          - name: "startup_time"
            command: "time docker run --rm alpine:latest echo 'hello'"
          - name: "runtime_overhead"
            command: "docker stats --no-stream"
    
    architecture_profiles:
      amd64:
        expected_performance:
          cpu_score: 1000
          memory_bandwidth: "20GB/s"
          storage_iops: 10000
          network_bandwidth: "1Gbps"
        power_efficiency: "medium"
        cost_efficiency: "low"
      
      arm64:
        expected_performance:
          cpu_score: 400
          memory_bandwidth: "8GB/s"
          storage_iops: 5000
          network_bandwidth: "1Gbps"
        power_efficiency: "high"
        cost_efficiency: "high"
      
      arm:
        expected_performance:
          cpu_score: 100
          memory_bandwidth: "2GB/s"
          storage_iops: 1000
          network_bandwidth: "100Mbps"
        power_efficiency: "very_high"
        cost_efficiency: "very_high"

  matrix_benchmark.py: |
    #!/usr/bin/env python3
    import numpy as np
    import time
    import json
    import sys
    
    def matrix_benchmark(size=1000, iterations=10):
        """Benchmark matrix multiplication performance"""
        print(f"Running matrix benchmark: {size}x{size} matrices, {iterations} iterations")
        
        results = []
        
        for i in range(iterations):
            # Generate random matrices
            A = np.random.rand(size, size).astype(np.float32)
            B = np.random.rand(size, size).astype(np.float32)
            
            # Time matrix multiplication
            start_time = time.time()
            C = np.dot(A, B)
            end_time = time.time()
            
            duration = end_time - start_time
            gflops = (2 * size ** 3) / (duration * 1e9)  # Approximate GFLOPS
            
            results.append({
                'iteration': i + 1,
                'duration_seconds': duration,
                'gflops': gflops
            })
            
            print(f"Iteration {i+1}: {duration:.3f}s, {gflops:.2f} GFLOPS")
        
        # Calculate statistics
        durations = [r['duration_seconds'] for r in results]
        gflops_values = [r['gflops'] for r in results]
        
        stats = {
            'avg_duration': np.mean(durations),
            'min_duration': np.min(durations),
            'max_duration': np.max(durations),
            'avg_gflops': np.mean(gflops_values),
            'max_gflops': np.max(gflops_values),
            'std_gflops': np.std(gflops_values)
        }
        
        print(f"\nBenchmark Results:")
        print(f"Average Duration: {stats['avg_duration']:.3f}s")
        print(f"Average Performance: {stats['avg_gflops']:.2f} GFLOPS")
        print(f"Peak Performance: {stats['max_gflops']:.2f} GFLOPS")
        
        return stats
    
    if __name__ == "__main__":
        size = int(sys.argv[1]) if len(sys.argv) > 1 else 1000
        iterations = int(sys.argv[2]) if len(sys.argv) > 2 else 10
        
        result = matrix_benchmark(size, iterations)
        
        # Output JSON for processing
        print("\nJSON_RESULT:", json.dumps(result))

  benchmark_runner.py: |
    #!/usr/bin/env python3
    import asyncio
    import json
    import logging
    import os
    import subprocess
    import time
    import yaml
    from datetime import datetime
    from typing import Dict, List, Optional
    from kubernetes import client, config
    
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    
    class PerformanceBenchmarker:
        def __init__(self):
            try:
                config.load_incluster_config()
            except:
                config.load_kube_config()
            
            self.v1 = client.CoreV1Api()
            self.batch_v1 = client.BatchV1Api()
            
            # Load benchmark configuration
            with open('/config/benchmark-suite.yaml', 'r') as f:
                self.config = yaml.safe_load(f)
            
            self.results = {}
            
        def get_node_info(self, node_name: str) -> Dict:
            """Get detailed node information"""
            node = self.v1.read_node(node_name)
            
            labels = node.metadata.labels or {}
            capacity = node.status.capacity or {}
            
            return {
                'name': node_name,
                'architecture': labels.get('kubernetes.io/arch', 'unknown'),
                'os': labels.get('kubernetes.io/os', 'unknown'),
                'instance_type': labels.get('node.kubernetes.io/instance-type', 'unknown'),
                'cpu_cores': capacity.get('cpu', '0'),
                'memory': capacity.get('memory', '0'),
                'storage': capacity.get('ephemeral-storage', '0'),
                'kernel_version': node.status.node_info.kernel_version,
                'container_runtime': node.status.node_info.container_runtime_version
            }
        
        def run_benchmark(self, node_name: str, benchmark_name: str, variant: str) -> Dict:
            """Run a specific benchmark on a node"""
            logger.info(f"Running {benchmark_name}/{variant} on {node_name}")
            
            benchmark_config = self.config['benchmarks'][benchmark_name]
            variant_config = next(v for v in benchmark_config['variants'] if v['name'] == variant)
            
            # Create a job to run the benchmark
            job_name = f"benchmark-{benchmark_name}-{variant}-{int(time.time())}"
            
            job_manifest = {
                'apiVersion': 'batch/v1',
                'kind': 'Job',
                'metadata': {
                    'name': job_name,
                    'namespace': 'benchmarking'
                },
                'spec': {
                    'template': {
                        'spec': {
                            'nodeSelector': {
                                'kubernetes.io/hostname': node_name
                            },
                            'restartPolicy': 'Never',
                            'containers': [
                                {
                                    'name': 'benchmark',
                                    'image': 'homelab/benchmark-runner:latest',
                                    'command': ['/bin/bash'],
                                    'args': [
                                        '-c',
                                        f"""
                                        echo "Starting benchmark: {benchmark_name}/{variant}"
                                        echo "Node: $(hostname)"
                                        echo "Architecture: $(uname -m)"
                                        echo "Start time: $(date)"
                                        
                                        # Install required tools
                                        apt-get update && apt-get install -y sysbench iperf3 python3-numpy
                                        
                                        # Run the benchmark
                                        START_TIME=$(date +%s.%N)
                                        {variant_config['command']}
                                        END_TIME=$(date +%s.%N)
                                        
                                        DURATION=$(echo "$END_TIME - $START_TIME" | bc)
                                        echo "Benchmark completed in $DURATION seconds"
                                        """
                                    ],
                                    'resources': {
                                        'requests': {
                                            'cpu': '100m',
                                            'memory': '256Mi'
                                        },
                                        'limits': {
                                            'cpu': '4000m',
                                            'memory': '2Gi'
                                        }
                                    }
                                }
                            ]
                        }
                    },
                    'backoffLimit': 1,
                    'activeDeadlineSeconds': int(benchmark_config['duration'][:-1])
                }
            }
            
            # Submit the job
            try:
                job = self.batch_v1.create_namespaced_job(
                    namespace='benchmarking',
                    body=job_manifest
                )
                
                # Wait for completion
                return self.wait_for_job_completion(job_name)
                
            except Exception as e:
                logger.error(f"Failed to run benchmark {benchmark_name}/{variant}: {e}")
                return {'status': 'failed', 'error': str(e)}
        
        def wait_for_job_completion(self, job_name: str, timeout: int = 600) -> Dict:
            """Wait for a job to complete and collect results"""
            start_time = time.time()
            
            while time.time() - start_time < timeout:
                try:
                    job = self.batch_v1.read_namespaced_job(
                        name=job_name,
                        namespace='benchmarking'
                    )
                    
                    if job.status.succeeded:
                        # Get job logs
                        pods = self.v1.list_namespaced_pod(
                            namespace='benchmarking',
                            label_selector=f'job-name={job_name}'
                        )
                        
                        if pods.items:
                            pod_name = pods.items[0].metadata.name
                            logs = self.v1.read_namespaced_pod_log(
                                name=pod_name,
                                namespace='benchmarking'
                            )
                            
                            # Parse results from logs
                            result = self.parse_benchmark_logs(logs)
                            result['status'] = 'completed'
                            result['job_name'] = job_name
                            
                            # Clean up job
                            self.batch_v1.delete_namespaced_job(
                                name=job_name,
                                namespace='benchmarking'
                            )
                            
                            return result
                    
                    elif job.status.failed:
                        return {
                            'status': 'failed',
                            'job_name': job_name,
                            'error': 'Job failed'
                        }
                    
                except Exception as e:
                    logger.error(f"Error checking job {job_name}: {e}")
                
                time.sleep(5)
            
            return {
                'status': 'timeout',
                'job_name': job_name,
                'error': 'Job timed out'
            }
        
        def parse_benchmark_logs(self, logs: str) -> Dict:
            """Parse benchmark results from job logs"""
            result = {
                'raw_logs': logs,
                'parsed_results': {}
            }
            
            lines = logs.split('\n')
            
            for line in lines:
                # Parse sysbench output
                if 'total time:' in line:
                    result['parsed_results']['total_time'] = line.split(':')[1].strip()
                elif 'total number of events:' in line:
                    result['parsed_results']['events'] = int(line.split(':')[1].strip())
                elif 'events per second:' in line:
                    result['parsed_results']['events_per_second'] = float(line.split(':')[1].strip())
                
                # Parse iperf3 output
                elif 'Mbits/sec' in line and 'receiver' in line:
                    parts = line.split()
                    if len(parts) >= 7:
                        result['parsed_results']['bandwidth_mbps'] = float(parts[6])
                
                # Parse custom benchmark JSON output
                elif line.startswith('JSON_RESULT:'):
                    try:
                        json_data = json.loads(line[12:])
                        result['parsed_results'].update(json_data)
                    except:
                        pass
            
            return result
        
        async def run_comprehensive_benchmark(self, target_nodes: List[str] = None) -> Dict:
            """Run comprehensive benchmark across specified nodes"""
            logger.info("Starting comprehensive performance benchmark")
            
            if target_nodes is None:
                # Get all nodes
                nodes = self.v1.list_node()
                target_nodes = [node.metadata.name for node in nodes.items]
            
            results = {
                'timestamp': datetime.now().isoformat(),
                'node_results': {},
                'summary': {}
            }
            
            for node_name in target_nodes:
                logger.info(f"Benchmarking node: {node_name}")
                
                node_info = self.get_node_info(node_name)
                arch = node_info['architecture']
                
                node_results = {
                    'node_info': node_info,
                    'benchmark_results': {},
                    'architecture': arch
                }
                
                # Run all benchmarks for this node
                for benchmark_name, benchmark_config in self.config['benchmarks'].items():
                    benchmark_results = {}
                    
                    for variant in benchmark_config['variants']:
                        variant_name = variant['name']
                        
                        try:
                            result = self.run_benchmark(node_name, benchmark_name, variant_name)
                            benchmark_results[variant_name] = result
                            
                            # Add small delay between benchmarks
                            await asyncio.sleep(10)
                            
                        except Exception as e:
                            logger.error(f"Error running {benchmark_name}/{variant_name} on {node_name}: {e}")
                            benchmark_results[variant_name] = {
                                'status': 'error',
                                'error': str(e)
                            }
                    
                    node_results['benchmark_results'][benchmark_name] = benchmark_results
                
                results['node_results'][node_name] = node_results
            
            # Generate summary
            results['summary'] = self.generate_benchmark_summary(results['node_results'])
            
            # Store results
            self.store_results(results)
            
            return results
        
        def generate_benchmark_summary(self, node_results: Dict) -> Dict:
            """Generate summary of benchmark results"""
            summary = {
                'by_architecture': {},
                'performance_rankings': {},
                'efficiency_metrics': {}
            }
            
            # Group results by architecture
            arch_results = {}
            for node_name, node_data in node_results.items():
                arch = node_data['architecture']
                if arch not in arch_results:
                    arch_results[arch] = []
                arch_results[arch].append(node_data)
            
            # Calculate architecture averages
            for arch, nodes in arch_results.items():
                arch_summary = {
                    'node_count': len(nodes),
                    'avg_performance': {},
                    'best_performance': {},
                    'power_efficiency': self.config['architecture_profiles'].get(arch, {}).get('power_efficiency', 'unknown')
                }
                
                # Aggregate performance metrics
                for benchmark_name in self.config['benchmarks'].keys():
                    arch_summary['avg_performance'][benchmark_name] = {}
                    arch_summary['best_performance'][benchmark_name] = {}
                
                summary['by_architecture'][arch] = arch_summary
            
            return summary
        
        def store_results(self, results: Dict):
            """Store benchmark results"""
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'/tmp/benchmark_results_{timestamp}.json'
            
            with open(filename, 'w') as f:
                json.dump(results, f, indent=2)
            
            logger.info(f"Benchmark results stored to {filename}")
            
            # Also store in ConfigMap for cluster access
            try:
                config_map = {
                    'apiVersion': 'v1',
                    'kind': 'ConfigMap',
                    'metadata': {
                        'name': f'benchmark-results-{timestamp}',
                        'namespace': 'benchmarking'
                    },
                    'data': {
                        'results.json': json.dumps(results, indent=2)
                    }
                }
                
                self.v1.create_namespaced_config_map(
                    namespace='benchmarking',
                    body=config_map
                )
                
                logger.info(f"Results stored in ConfigMap: benchmark-results-{timestamp}")
                
            except Exception as e:
                logger.error(f"Failed to store results in ConfigMap: {e}")
    
    if __name__ == "__main__":
        benchmarker = PerformanceBenchmarker()
        
        # Run comprehensive benchmark
        results = asyncio.run(benchmarker.run_comprehensive_benchmark())
        
        print("Benchmark completed!")
        print(f"Results: {json.dumps(results['summary'], indent=2)}")

---
# Benchmark runner deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: benchmark-controller
  namespace: benchmarking
spec:
  replicas: 1
  selector:
    matchLabels:
      app: benchmark-controller
  template:
    metadata:
      labels:
        app: benchmark-controller
    spec:
      nodeSelector:
        kubernetes.io/arch: amd64  # Run on powerful node
      serviceAccountName: benchmark-controller
      containers:
      - name: controller
        image: python:3.11-slim
        command: ["/bin/bash"]
        args:
        - -c
        - |
          pip install kubernetes pyyaml asyncio
          python3 /app/benchmark_runner.py
        volumeMounts:
        - name: benchmark-app
          mountPath: /app
        - name: benchmark-config
          mountPath: /config
        env:
        - name: PYTHONUNBUFFERED
          value: "1"
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        ports:
        - containerPort: 8080
          name: api
      volumes:
      - name: benchmark-app
        configMap:
          name: benchmark-config
          items:
          - key: benchmark_runner.py
            path: benchmark_runner.py
          - key: matrix_benchmark.py
            path: matrix_benchmark.py
      - name: benchmark-config
        configMap:
          name: benchmark-config

---
# Benchmark runner image builder
apiVersion: batch/v1
kind: Job
metadata:
  name: build-benchmark-image
  namespace: benchmarking
spec:
  template:
    spec:
      nodeSelector:
        kubernetes.io/arch: amd64
      restartPolicy: OnFailure
      containers:
      - name: builder
        image: docker:24-dind
        command: ["/bin/sh"]
        args:
        - -c
        - |
          dockerd &
          sleep 10
          
          # Create Dockerfile for benchmark runner
          cat > Dockerfile << 'EOF'
          FROM ubuntu:22.04
          
          RUN apt-get update && apt-get install -y \
              sysbench \
              iperf3 \
              python3 \
              python3-pip \
              python3-numpy \
              bc \
              curl \
              stress-ng \
              fio \
              && rm -rf /var/lib/apt/lists/*
          
          # Install additional Python packages
          RUN pip3 install scipy matplotlib
          
          # Copy benchmark scripts
          COPY benchmark_scripts/ /benchmarks/
          RUN chmod +x /benchmarks/*.py
          
          WORKDIR /benchmarks
          EOF
          
          # Create benchmark scripts directory
          mkdir -p benchmark_scripts
          
          # Copy matrix benchmark script
          cp /config/matrix_benchmark.py benchmark_scripts/
          
          # Build multi-arch image
          docker buildx create --name homelab-builder --use 2>/dev/null || docker buildx use homelab-builder
          docker buildx build \
            --platform linux/amd64,linux/arm64 \
            --tag homelab/benchmark-runner:latest \
            --push \
            .
          
          echo "Benchmark runner image built successfully"
        volumeMounts:
        - name: docker-socket
          mountPath: /var/run/docker.sock
        - name: benchmark-config
          mountPath: /config
        securityContext:
          privileged: true
      volumes:
      - name: docker-socket
        hostPath:
          path: /var/run/docker.sock
      - name: benchmark-config
        configMap:
          name: benchmark-config

---
# Performance monitoring dashboard
apiVersion: v1
kind: ConfigMap
metadata:
  name: benchmark-dashboard
  namespace: benchmarking
data:
  dashboard.html: |
    <!DOCTYPE html>
    <html>
    <head>
        <title>Homelab Performance Dashboard</title>
        <style>
            body { font-family: Arial, sans-serif; margin: 20px; background: #f5f5f5; }
            .container { max-width: 1200px; margin: 0 auto; }
            .card { background: white; padding: 20px; margin: 20px 0; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }
            .arch-section { border-left: 4px solid #007acc; }
            .metric { display: inline-block; margin: 10px; padding: 10px; background: #f0f8ff; border-radius: 4px; }
            .performance-chart { width: 100%; height: 300px; background: #fafafa; border: 1px solid #ddd; }
            .arch-amd64 { color: #ff6b35; }
            .arch-arm64 { color: #1b998b; }
            .arch-arm { color: #6a4c93; }
            .status-good { color: #28a745; }
            .status-warning { color: #ffc107; }
            .status-bad { color: #dc3545; }
        </style>
    </head>
    <body>
        <div class="container">
            <h1>üè† Homelab Performance Dashboard</h1>
            
            <div class="card">
                <h2>üìä Architecture Comparison</h2>
                <div class="arch-section">
                    <h3 class="arch-amd64">AMD64 Nodes (Compute)</h3>
                    <div class="metric">CPU Performance: <span class="status-good">High</span></div>
                    <div class="metric">Memory Bandwidth: <span class="status-good">20GB/s</span></div>
                    <div class="metric">Power Efficiency: <span class="status-warning">Medium</span></div>
                    <div class="metric">Cost Efficiency: <span class="status-bad">Low</span></div>
                </div>
                
                <div class="arch-section">
                    <h3 class="arch-arm64">ARM64 Nodes (Edge)</h3>
                    <div class="metric">CPU Performance: <span class="status-warning">Medium</span></div>
                    <div class="metric">Memory Bandwidth: <span class="status-warning">8GB/s</span></div>
                    <div class="metric">Power Efficiency: <span class="status-good">High</span></div>
                    <div class="metric">Cost Efficiency: <span class="status-good">High</span></div>
                </div>
                
                <div class="arch-section">
                    <h3 class="arch-arm">ARM Nodes (Sensors)</h3>
                    <div class="metric">CPU Performance: <span class="status-bad">Low</span></div>
                    <div class="metric">Memory Bandwidth: <span class="status-bad">2GB/s</span></div>
                    <div class="metric">Power Efficiency: <span class="status-good">Very High</span></div>
                    <div class="metric">Cost Efficiency: <span class="status-good">Very High</span></div>
                </div>
            </div>
            
            <div class="card">
                <h2>‚ö° Power Consumption Analysis</h2>
                <div class="performance-chart">
                    <p>Power consumption by architecture:</p>
                    <ul>
                        <li class="arch-amd64">AMD64: ~45W per node (2 nodes = 90W)</li>
                        <li class="arch-arm64">ARM64: ~7W per node (2 nodes = 14W)</li>
                        <li class="arch-arm">ARM: ~3W per node (2 nodes = 6W)</li>
                    </ul>
                    <p><strong>Total Cluster Power: ~110W</strong></p>
                    <p>Efficiency: 4.5x more efficient than all-AMD64 cluster</p>
                </div>
            </div>
            
            <div class="card">
                <h2>üìà Performance Trends</h2>
                <div id="benchmark-results">
                    <p>Latest benchmark results will be loaded here...</p>
                </div>
            </div>
            
            <div class="card">
                <h2>üéØ Workload Optimization Recommendations</h2>
                <ul>
                    <li><strong>Heavy Compute:</strong> Route to AMD64 nodes for maximum performance</li>
                    <li><strong>Web Services:</strong> Deploy on ARM64 for optimal power/performance balance</li>
                    <li><strong>IoT/Sensors:</strong> Use ARM nodes for ultra-low power consumption</li>
                    <li><strong>Databases:</strong> Prefer AMD64 for memory bandwidth requirements</li>
                    <li><strong>Edge AI:</strong> ARM64 provides good inference performance per watt</li>
                </ul>
            </div>
        </div>
        
        <script>
            // Load benchmark results
            async function loadBenchmarkResults() {
                try {
                    const response = await fetch('/api/benchmark-results');
                    const data = await response.json();
                    
                    const resultsDiv = document.getElementById('benchmark-results');
                    resultsDiv.innerHTML = '<pre>' + JSON.stringify(data, null, 2) + '</pre>';
                } catch (error) {
                    console.error('Failed to load benchmark results:', error);
                }
            }
            
            // Load results on page load
            loadBenchmarkResults();
            
            // Refresh every 5 minutes
            setInterval(loadBenchmarkResults, 300000);
        </script>
    </body>
    </html>

---
# Performance monitoring CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: performance-monitor
  namespace: benchmarking
spec:
  schedule: "0 */6 * * *"  # Every 6 hours
  jobTemplate:
    spec:
      template:
        spec:
          nodeSelector:
            kubernetes.io/arch: amd64
          restartPolicy: OnFailure
          containers:
          - name: monitor
            image: bitnami/kubectl:latest
            command: ["/bin/bash"]
            args:
            - -c
            - |
              echo "Running performance monitoring check..."
              
              # Check cluster resource usage
              echo "=== Cluster Resource Usage ==="
              kubectl top nodes --sort-by=cpu
              echo ""
              
              # Check resource usage by architecture
              echo "=== Resource Usage by Architecture ==="
              
              # AMD64 nodes
              echo "AMD64 Nodes:"
              kubectl top nodes -l kubernetes.io/arch=amd64 --no-headers | while read line; do
                echo "  $line"
              done
              
              # ARM64 nodes
              echo "ARM64 Nodes:"
              kubectl top nodes -l kubernetes.io/arch=arm64 --no-headers | while read line; do
                echo "  $line"
              done
              
              # ARM nodes
              echo "ARM Nodes:"
              kubectl top nodes -l kubernetes.io/arch=arm --no-headers | while read line; do
                echo "  $line"
              done
              
              # Check for performance issues
              echo "=== Performance Health Check ==="
              
              # High CPU usage alert
              HIGH_CPU_NODES=$(kubectl top nodes --no-headers | awk '$3 > 80 {print $1}')
              if [ -n "$HIGH_CPU_NODES" ]; then
                echo "WARNING: High CPU usage detected on nodes: $HIGH_CPU_NODES"
              fi
              
              # High memory usage alert
              HIGH_MEM_NODES=$(kubectl top nodes --no-headers | awk '$5 > 80 {print $1}')
              if [ -n "$HIGH_MEM_NODES" ]; then
                echo "WARNING: High memory usage detected on nodes: $HIGH_MEM_NODES"
              fi
              
              # Check pod distribution
              echo "=== Pod Distribution by Architecture ==="
              kubectl get pods --all-namespaces -o wide | \
                awk 'NR>1 {print $8}' | sort | uniq -c | \
                while read count node; do
                  arch=$(kubectl get node $node -o jsonpath='{.metadata.labels.kubernetes\.io/arch}' 2>/dev/null || echo "unknown")
                  echo "$arch: $count pods on $node"
                done
              
              echo "Performance monitoring completed"
            resources:
              requests:
                cpu: "50m"
                memory: "64Mi"
              limits:
                cpu: "200m"
                memory: "128Mi"

---
# Benchmark runner service account
apiVersion: v1
kind: ServiceAccount
metadata:
  name: benchmark-controller
  namespace: benchmarking

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: benchmark-controller
rules:
- apiGroups: [""]
  resources: ["nodes", "pods", "configmaps"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["batch"]
  resources: ["jobs"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["metrics.k8s.io"]
  resources: ["nodes", "pods"]
  verbs: ["get", "list"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: benchmark-controller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: benchmark-controller
subjects:
- kind: ServiceAccount
  name: benchmark-controller
  namespace: benchmarking

---
# Example benchmark execution
apiVersion: batch/v1
kind: Job
metadata:
  name: run-initial-benchmark
  namespace: benchmarking
spec:
  template:
    spec:
      nodeSelector:
        kubernetes.io/arch: amd64
      restartPolicy: OnFailure
      containers:
      - name: benchmark
        image: python:3.11-slim
        command: ["/bin/bash"]
        args:
        - -c
        - |
          pip install kubernetes pyyaml
          
          echo "Running initial performance benchmark suite..."
          
          # This would trigger the comprehensive benchmark
          python3 -c "
          import asyncio
          import sys
          sys.path.append('/app')
          
          # Simulate benchmark execution
          print('Benchmark suite would run here...')
          print('Results: Comprehensive performance analysis completed')
          "
        volumeMounts:
        - name: benchmark-config
          mountPath: /app
      volumes:
      - name: benchmark-config
        configMap:
          name: benchmark-config