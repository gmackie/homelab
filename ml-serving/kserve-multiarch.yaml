# KServe ML Model Serving for Multi-Architecture Homelab
---
apiVersion: v1
kind: Namespace
metadata:
  name: ml-serving
  labels:
    name: ml-serving

---
# KServe configuration for multi-arch ML serving
apiVersion: v1
kind: ConfigMap
metadata:
  name: kserve-config
  namespace: ml-serving
data:
  config.yaml: |
    # KServe configuration for multi-architecture deployment
    inference:
      storageUri: "s3://ml-models/"
      batchTimeout: "30s"
      maxBatchSize: 32
      
    architectures:
      amd64:
        frameworks:
          tensorflow: "tensorflow/serving:2.13.0"
          pytorch: "pytorch/torchserve:0.8.2-cpu"
          sklearn: "python:3.11-slim"
          xgboost: "python:3.11-slim"
        resources:
          cpu: "2000m"
          memory: "4Gi"
        accelerators: [] # No GPU in homelab
        
      arm64:
        frameworks:
          tensorflow: "tensorflow/serving:2.13.0"  # TensorFlow supports ARM64
          pytorch: "pytorch/torchserve:0.8.2-cpu"
          sklearn: "python:3.11-slim"
          lightgbm: "python:3.11-slim"  # Lightweight ML on ARM64
        resources:
          cpu: "1000m"
          memory: "2Gi"
        accelerators: []
        
      arm:
        frameworks:
          sklearn: "python:3.11-slim"
          lightgbm: "python:3.11-slim"  # Only lightweight models on ARM
          edge-ml: "python:3.11-slim"   # Custom edge ML models
        resources:
          cpu: "500m"
          memory: "1Gi"
        accelerators: []
        
    deployment_strategy:
      inference_preference:
        - name: "heavy_models"
          architectures: ["amd64"]
          frameworks: ["tensorflow", "pytorch"]
          
        - name: "lightweight_models"
          architectures: ["arm64", "arm"]
          frameworks: ["sklearn", "lightgbm"]
          
        - name: "edge_inference"
          architectures: ["arm"]
          frameworks: ["edge-ml"]
          
    auto_scaling:
      minReplicas: 1
      maxReplicas: 10
      targetConcurrency: 100
      architecture_weights:
        amd64: 1.0
        arm64: 0.7
        arm: 0.5

---
# Model storage backend (MinIO for homelab)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: minio
  namespace: ml-serving
spec:
  replicas: 1
  selector:
    matchLabels:
      app: minio
  template:
    metadata:
      labels:
        app: minio
    spec:
      # Prefer AMD64 for storage performance
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: kubernetes.io/arch
                operator: In
                values: ["amd64"]
      containers:
      - name: minio
        image: minio/minio:latest
        args:
        - server
        - /data
        - --console-address
        - ":9001"
        env:
        - name: MINIO_ROOT_USER
          value: "admin"
        - name: MINIO_ROOT_PASSWORD
          value: "minio123"
        ports:
        - containerPort: 9000
          name: api
        - containerPort: 9001
          name: console
        resources:
          requests:
            cpu: "200m"
            memory: "512Mi"
          limits:
            cpu: "1000m"
            memory: "2Gi"
        volumeMounts:
        - name: minio-data
          mountPath: /data
        livenessProbe:
          httpGet:
            path: /minio/health/live
            port: 9000
        readinessProbe:
          httpGet:
            path: /minio/health/ready
            port: 9000
      volumes:
      - name: minio-data
        persistentVolumeClaim:
          claimName: minio-storage

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: minio-storage
  namespace: ml-serving
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
  storageClassName: longhorn-ssd

---
# Model registry service
apiVersion: apps/v1
kind: Deployment
metadata:
  name: model-registry
  namespace: ml-serving
spec:
  replicas: 1
  selector:
    matchLabels:
      app: model-registry
  template:
    metadata:
      labels:
        app: model-registry
    spec:
      serviceAccountName: model-registry
      containers:
      - name: registry
        image: python:3.11-slim
        command: ["/bin/bash"]
        args:
        - -c
        - |
          pip install fastapi uvicorn minio boto3 pydantic sqlalchemy psycopg2-binary
          python3 /app/model_registry.py
        volumeMounts:
        - name: registry-app
          mountPath: /app
        env:
        - name: MINIO_ENDPOINT
          value: "minio:9000"
        - name: MINIO_ACCESS_KEY
          value: "admin"
        - name: MINIO_SECRET_KEY
          value: "minio123"
        ports:
        - containerPort: 8000
        resources:
          requests:
            cpu: "100m"
            memory: "256Mi"
          limits:
            cpu: "500m"
            memory: "512Mi"
      volumes:
      - name: registry-app
        configMap:
          name: model-registry-app

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: model-registry-app
  namespace: ml-serving
data:
  model_registry.py: |
    #!/usr/bin/env python3
    import os
    import json
    import logging
    from datetime import datetime
    from typing import Dict, List, Optional
    from fastapi import FastAPI, HTTPException
    from pydantic import BaseModel
    from minio import Minio
    import uvicorn
    
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    
    app = FastAPI(title="Multi-Arch ML Model Registry")
    
    # MinIO client
    minio_client = Minio(
        os.getenv("MINIO_ENDPOINT", "minio:9000"),
        access_key=os.getenv("MINIO_ACCESS_KEY", "admin"),
        secret_key=os.getenv("MINIO_SECRET_KEY", "minio123"),
        secure=False
    )
    
    # Ensure bucket exists
    bucket_name = "ml-models"
    if not minio_client.bucket_exists(bucket_name):
        minio_client.make_bucket(bucket_name)
        logger.info(f"Created bucket: {bucket_name}")
    
    class ModelMetadata(BaseModel):
        name: str
        version: str
        framework: str
        architecture: str
        description: Optional[str] = ""
        tags: List[str] = []
        model_size_mb: Optional[float] = None
        performance_metrics: Dict = {}
        created_at: datetime = datetime.now()
        
    class ArchitectureRecommendation(BaseModel):
        model_name: str
        recommended_arch: str
        reason: str
        expected_latency_ms: float
        expected_throughput_qps: float
        power_efficiency: float
    
    # In-memory model registry (in production, use proper database)
    model_registry = {}
    
    @app.get("/")
    def health_check():
        return {"status": "healthy", "service": "model-registry"}
    
    @app.post("/models", response_model=ModelMetadata)
    async def register_model(model: ModelMetadata):
        """Register a new model"""
        model_id = f"{model.name}:{model.version}"
        
        # Validate architecture
        valid_archs = ["amd64", "arm64", "arm", "multi-arch"]
        if model.architecture not in valid_archs:
            raise HTTPException(400, f"Invalid architecture. Must be one of: {valid_archs}")
        
        # Store metadata
        model_registry[model_id] = model.dict()
        
        logger.info(f"Registered model: {model_id} for {model.architecture}")
        return model
    
    @app.get("/models", response_model=List[ModelMetadata])
    def list_models():
        """List all registered models"""
        return [ModelMetadata(**model) for model in model_registry.values()]
    
    @app.get("/models/{name}", response_model=List[ModelMetadata])
    def get_model_versions(name: str):
        """Get all versions of a specific model"""
        versions = [
            ModelMetadata(**model) for model_id, model in model_registry.items()
            if model_id.split(":")[0] == name
        ]
        if not versions:
            raise HTTPException(404, f"Model {name} not found")
        return versions
    
    @app.get("/models/{name}/{version}", response_model=ModelMetadata)
    def get_model(name: str, version: str):
        """Get specific model version"""
        model_id = f"{name}:{version}"
        if model_id not in model_registry:
            raise HTTPException(404, f"Model {model_id} not found")
        return ModelMetadata(**model_registry[model_id])
    
    @app.get("/recommend/{model_name}", response_model=ArchitectureRecommendation)
    def recommend_architecture(model_name: str, target_latency_ms: Optional[float] = None, target_qps: Optional[float] = None):
        """Recommend best architecture for model deployment"""
        
        # Find model
        model_versions = [
            model for model_id, model in model_registry.items()
            if model_id.split(":")[0] == model_name
        ]
        
        if not model_versions:
            raise HTTPException(404, f"Model {model_name} not found")
        
        # Get latest version
        latest_model = max(model_versions, key=lambda x: x["created_at"])
        framework = latest_model["framework"]
        
        # Architecture recommendation logic
        recommendations = {
            "tensorflow": {
                "arch": "amd64",
                "latency": 50.0,
                "throughput": 20.0,
                "efficiency": 0.44  # requests per watt
            },
            "pytorch": {
                "arch": "amd64", 
                "latency": 45.0,
                "throughput": 22.0,
                "efficiency": 0.49
            },
            "sklearn": {
                "arch": "arm64",
                "latency": 15.0,
                "throughput": 50.0,
                "efficiency": 7.14  # Much better efficiency on ARM64
            },
            "lightgbm": {
                "arch": "arm64",
                "latency": 10.0,
                "throughput": 80.0,
                "efficiency": 11.43
            },
            "edge-ml": {
                "arch": "arm",
                "latency": 5.0,
                "throughput": 100.0,
                "efficiency": 33.33  # Best efficiency on ARM
            }
        }
        
        if framework not in recommendations:
            # Default to AMD64 for unknown frameworks
            rec = {
                "arch": "amd64",
                "latency": 100.0,
                "throughput": 10.0,
                "efficiency": 0.22
            }
        else:
            rec = recommendations[framework]
        
        # Adjust recommendation based on constraints
        reason = f"Optimal for {framework} framework"
        if target_latency_ms and target_latency_ms < rec["latency"]:
            if framework in ["sklearn", "lightgbm"]:
                rec["arch"] = "amd64"
                rec["latency"] = target_latency_ms * 0.8
                reason = f"AMD64 recommended for strict latency requirement ({target_latency_ms}ms)"
        
        return ArchitectureRecommendation(
            model_name=model_name,
            recommended_arch=rec["arch"],
            reason=reason,
            expected_latency_ms=rec["latency"],
            expected_throughput_qps=rec["throughput"],
            power_efficiency=rec["efficiency"]
        )
    
    @app.get("/metrics")
    def get_metrics():
        """Get registry metrics"""
        arch_counts = {}
        framework_counts = {}
        
        for model in model_registry.values():
            arch = model["architecture"]
            framework = model["framework"]
            
            arch_counts[arch] = arch_counts.get(arch, 0) + 1
            framework_counts[framework] = framework_counts.get(framework, 0) + 1
        
        return {
            "total_models": len(model_registry),
            "by_architecture": arch_counts,
            "by_framework": framework_counts,
            "storage_bucket": bucket_name
        }
    
    if __name__ == "__main__":
        uvicorn.run(app, host="0.0.0.0", port=8000)

---
# TensorFlow Serving example (AMD64 optimized)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tensorflow-serving-amd64
  namespace: ml-serving
  labels:
    app: tensorflow-serving
    architecture: amd64
spec:
  replicas: 1
  selector:
    matchLabels:
      app: tensorflow-serving
      architecture: amd64
  template:
    metadata:
      labels:
        app: tensorflow-serving
        architecture: amd64
      annotations:
        linkerd.io/inject: enabled
        prometheus.io/scrape: "true"
        prometheus.io/port: "8501"
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/arch
                operator: In
                values: ["amd64"]
      containers:
      - name: tensorflow-serving
        image: tensorflow/serving:2.13.0
        args:
        - "--model_config_file=/models/model_config.txt"
        - "--monitoring_config_file=/models/monitoring_config.txt"
        - "--allow_version_labels=true"
        - "--model_config_file_poll_wait_seconds=60"
        env:
        - name: TF_CPP_MIN_LOG_LEVEL
          value: "2"
        ports:
        - containerPort: 8500
          name: grpc
        - containerPort: 8501
          name: http
        resources:
          requests:
            cpu: "1000m"
            memory: "2Gi"
          limits:
            cpu: "4000m"
            memory: "8Gi"
        volumeMounts:
        - name: model-config
          mountPath: /models
        - name: model-storage
          mountPath: /mnt/models
        livenessProbe:
          httpGet:
            path: /v1/models
            port: 8501
        readinessProbe:
          httpGet:
            path: /v1/models
            port: 8501
      volumes:
      - name: model-config
        configMap:
          name: tensorflow-config
      - name: model-storage
        persistentVolumeClaim:
          claimName: tensorflow-models

---
# Scikit-learn serving on ARM64 (power efficient)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sklearn-serving-arm64
  namespace: ml-serving
  labels:
    app: sklearn-serving
    architecture: arm64
spec:
  replicas: 2
  selector:
    matchLabels:
      app: sklearn-serving
      architecture: arm64
  template:
    metadata:
      labels:
        app: sklearn-serving
        architecture: arm64
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/arch
                operator: In
                values: ["arm64"]
      containers:
      - name: sklearn-server
        image: python:3.11-slim
        command: ["/bin/bash"]
        args:
        - -c
        - |
          pip install fastapi uvicorn scikit-learn pandas numpy joblib prometheus-client
          python3 /app/sklearn_server.py
        volumeMounts:
        - name: sklearn-app
          mountPath: /app
        - name: sklearn-models
          mountPath: /models
        ports:
        - containerPort: 8000
          name: http
        - containerPort: 8080
          name: metrics
        env:
        - name: MODEL_PATH
          value: "/models"
        - name: ARCHITECTURE
          value: "arm64"
        resources:
          requests:
            cpu: "200m"
            memory: "512Mi"
          limits:
            cpu: "1000m"
            memory: "2Gi"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
      volumes:
      - name: sklearn-app
        configMap:
          name: sklearn-server-app
      - name: sklearn-models
        persistentVolumeClaim:
          claimName: sklearn-models

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: sklearn-server-app
  namespace: ml-serving
data:
  sklearn_server.py: |
    #!/usr/bin/env python3
    import os
    import json
    import logging
    import joblib
    import numpy as np
    import pandas as pd
    from fastapi import FastAPI, HTTPException
    from pydantic import BaseModel
    from typing import List, Dict, Any
    from prometheus_client import Counter, Histogram, Gauge, generate_latest
    from fastapi.responses import Response
    import uvicorn
    import glob
    import time
    
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    
    # Prometheus metrics
    REQUEST_COUNT = Counter('sklearn_requests_total', 'Total requests', ['model', 'method'])
    REQUEST_LATENCY = Histogram('sklearn_request_duration_seconds', 'Request latency', ['model'])
    MODEL_LOAD_TIME = Gauge('sklearn_model_load_time_seconds', 'Model load time', ['model'])
    ACTIVE_MODELS = Gauge('sklearn_active_models', 'Number of active models')
    
    app = FastAPI(title="Scikit-learn Serving")
    
    # Model storage
    models = {}
    model_metadata = {}
    
    class PredictionRequest(BaseModel):
        instances: List[List[float]]
        model_name: Optional[str] = "default"
        
    class PredictionResponse(BaseModel):
        predictions: List[Any]
        model_name: str
        architecture: str = os.getenv("ARCHITECTURE", "unknown")
        latency_ms: float
    
    def load_models():
        """Load all models from model directory"""
        model_path = os.getenv("MODEL_PATH", "/models")
        
        if not os.path.exists(model_path):
            logger.warning(f"Model path {model_path} does not exist")
            return
        
        model_files = glob.glob(f"{model_path}/**/*.joblib", recursive=True)
        
        for model_file in model_files:
            model_name = os.path.basename(model_file).replace('.joblib', '')
            
            try:
                start_time = time.time()
                model = joblib.load(model_file)
                load_time = time.time() - start_time
                
                models[model_name] = model
                MODEL_LOAD_TIME.labels(model=model_name).set(load_time)
                
                # Try to load metadata
                metadata_file = model_file.replace('.joblib', '_metadata.json')
                if os.path.exists(metadata_file):
                    with open(metadata_file, 'r') as f:
                        model_metadata[model_name] = json.load(f)
                
                logger.info(f"Loaded model {model_name} in {load_time:.2f}s")
                
            except Exception as e:
                logger.error(f"Failed to load model {model_file}: {e}")
        
        ACTIVE_MODELS.set(len(models))
        logger.info(f"Loaded {len(models)} models: {list(models.keys())}")
    
    @app.on_event("startup")
    async def startup_event():
        load_models()
        
        # Create a simple default model if no models exist
        if not models:
            from sklearn.ensemble import RandomForestClassifier
            from sklearn.datasets import make_classification
            
            logger.info("No models found, creating default model")
            X, y = make_classification(n_samples=1000, n_features=20, n_classes=2)
            model = RandomForestClassifier(n_estimators=50)
            model.fit(X, y)
            
            models["default"] = model
            model_metadata["default"] = {
                "name": "default",
                "type": "RandomForestClassifier",
                "features": 20,
                "classes": 2,
                "created_for": "demo"
            }
            ACTIVE_MODELS.set(1)
    
    @app.get("/")
    def health_check():
        return {"status": "healthy", "models": list(models.keys())}
    
    @app.get("/health")
    def health():
        return {"status": "ok", "architecture": os.getenv("ARCHITECTURE", "unknown")}
    
    @app.get("/models")
    def list_models():
        """List available models"""
        model_list = []
        for name, model in models.items():
            metadata = model_metadata.get(name, {})
            model_list.append({
                "name": name,
                "type": type(model).__name__,
                "metadata": metadata
            })
        return {"models": model_list}
    
    @app.post("/predict", response_model=PredictionResponse)
    async def predict(request: PredictionRequest):
        """Make predictions"""
        start_time = time.time()
        
        model_name = request.model_name
        if model_name not in models:
            REQUEST_COUNT.labels(model=model_name, method="predict_error").inc()
            raise HTTPException(404, f"Model {model_name} not found")
        
        model = models[model_name]
        
        try:
            # Convert to numpy array
            instances = np.array(request.instances)
            
            # Make predictions
            predictions = model.predict(instances)
            
            # Convert numpy types to Python types for JSON serialization
            if hasattr(predictions, 'tolist'):
                predictions = predictions.tolist()
            else:
                predictions = [float(p) for p in predictions]
            
            latency = (time.time() - start_time) * 1000  # Convert to ms
            
            REQUEST_COUNT.labels(model=model_name, method="predict_success").inc()
            REQUEST_LATENCY.labels(model=model_name).observe(time.time() - start_time)
            
            return PredictionResponse(
                predictions=predictions,
                model_name=model_name,
                architecture=os.getenv("ARCHITECTURE", "unknown"),
                latency_ms=latency
            )
            
        except Exception as e:
            REQUEST_COUNT.labels(model=model_name, method="predict_error").inc()
            logger.error(f"Prediction error for model {model_name}: {e}")
            raise HTTPException(500, f"Prediction failed: {str(e)}")
    
    @app.get("/predict/batch/{model_name}")
    async def batch_predict(model_name: str, batch_size: int = 32):
        """Benchmark batch prediction performance"""
        if model_name not in models:
            raise HTTPException(404, f"Model {model_name} not found")
        
        model = models[model_name]
        
        # Generate random test data
        if hasattr(model, 'n_features_in_'):
            n_features = model.n_features_in_
        else:
            n_features = 20  # Default
        
        test_data = np.random.randn(batch_size, n_features)
        
        start_time = time.time()
        predictions = model.predict(test_data)
        latency = (time.time() - start_time) * 1000
        
        return {
            "model": model_name,
            "batch_size": batch_size,
            "latency_ms": latency,
            "throughput_qps": batch_size / (latency / 1000),
            "predictions_sample": predictions[:5].tolist() if hasattr(predictions, 'tolist') else predictions[:5]
        }
    
    @app.get("/metrics")
    def metrics():
        """Prometheus metrics endpoint"""
        return Response(generate_latest(), media_type="text/plain")
    
    @app.post("/reload")
    def reload_models():
        """Reload models from disk"""
        models.clear()
        model_metadata.clear()
        load_models()
        return {"status": "reloaded", "models": list(models.keys())}
    
    if __name__ == "__main__":
        uvicorn.run(app, host="0.0.0.0", port=8000)

---
# Edge ML serving on ARM (ultra-lightweight)
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: edge-ml-arm
  namespace: ml-serving
  labels:
    app: edge-ml
    architecture: arm
spec:
  selector:
    matchLabels:
      app: edge-ml
      architecture: arm
  template:
    metadata:
      labels:
        app: edge-ml
        architecture: arm
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/arch
                operator: In
                values: ["arm"]
      hostNetwork: true
      containers:
      - name: edge-ml
        image: python:3.11-alpine
        command: ["/bin/sh"]
        args:
        - -c
        - |
          pip install fastapi uvicorn numpy scikit-learn --no-cache-dir
          python3 /app/edge_ml.py
        volumeMounts:
        - name: edge-app
          mountPath: /app
        ports:
        - containerPort: 8000
          hostPort: 8000
          name: http
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        resources:
          requests:
            cpu: "100m"
            memory: "128Mi"
          limits:
            cpu: "250m"
            memory: "256Mi"
      volumes:
      - name: edge-app
        configMap:
          name: edge-ml-app

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: edge-ml-app
  namespace: ml-serving
data:
  edge_ml.py: |
    #!/usr/bin/env python3
    import os
    import json
    import logging
    import numpy as np
    from fastapi import FastAPI
    from pydantic import BaseModel
    from typing import List
    import uvicorn
    
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    
    app = FastAPI(title="Edge ML Serving")
    
    class EdgePredictionRequest(BaseModel):
        sensor_data: List[float]
        model_type: str = "anomaly_detection"
    
    class EdgePredictionResponse(BaseModel):
        prediction: float
        confidence: float
        node: str
        edge_latency_ms: float
    
    # Simple anomaly detection model (no external dependencies)
    class SimpleAnomalyDetector:
        def __init__(self):
            self.threshold = 2.0
            self.window_size = 10
            self.history = []
        
        def predict(self, data):
            # Add to history
            self.history.extend(data)
            if len(self.history) > self.window_size * 10:
                self.history = self.history[-self.window_size * 10:]
            
            if len(self.history) < self.window_size:
                return 0.0, 0.5  # Not enough data
            
            # Calculate z-score
            recent_data = np.array(self.history[-self.window_size:])
            mean = np.mean(recent_data)
            std = np.std(recent_data)
            
            if std == 0:
                return 0.0, 1.0
            
            current_value = data[-1] if data else 0
            z_score = abs((current_value - mean) / std)
            
            anomaly_score = min(z_score / self.threshold, 1.0)
            confidence = 1.0 - (1.0 / (1.0 + z_score))
            
            return anomaly_score, confidence
    
    # Initialize simple model
    detector = SimpleAnomalyDetector()
    
    @app.get("/")
    def health_check():
        return {
            "status": "healthy", 
            "node": os.getenv("NODE_NAME", "unknown"),
            "architecture": "arm",
            "model": "simple_anomaly_detector"
        }
    
    @app.post("/predict", response_model=EdgePredictionResponse)
    async def predict(request: EdgePredictionRequest):
        """Edge ML prediction with minimal latency"""
        import time
        start_time = time.time()
        
        try:
            if request.model_type == "anomaly_detection":
                score, confidence = detector.predict(request.sensor_data)
            else:
                # Default to simple threshold
                avg_value = np.mean(request.sensor_data)
                score = 1.0 if avg_value > 50 else 0.0
                confidence = 0.8
            
            latency = (time.time() - start_time) * 1000
            
            return EdgePredictionResponse(
                prediction=score,
                confidence=confidence,
                node=os.getenv("NODE_NAME", "unknown"),
                edge_latency_ms=latency
            )
            
        except Exception as e:
            logger.error(f"Edge prediction error: {e}")
            return EdgePredictionResponse(
                prediction=0.0,
                confidence=0.0,
                node=os.getenv("NODE_NAME", "unknown"),
                edge_latency_ms=0.0
            )
    
    @app.get("/metrics")
    def get_metrics():
        """Simple metrics for edge deployment"""
        return {
            "model_type": "anomaly_detection",
            "history_size": len(detector.history),
            "threshold": detector.threshold,
            "node": os.getenv("NODE_NAME", "unknown")
        }
    
    if __name__ == "__main__":
        uvicorn.run(app, host="0.0.0.0", port=8000)

---
# ML Model deployment automation
apiVersion: batch/v1
kind: Job
metadata:
  name: ml-model-deployer
  namespace: ml-serving
spec:
  template:
    spec:
      restartPolicy: OnFailure
      containers:
      - name: deployer
        image: python:3.11-slim
        command: ["/bin/bash"]
        args:
        - -c
        - |
          pip install requests scikit-learn joblib minio
          python3 /app/model_deployer.py
        volumeMounts:
        - name: deployer-app
          mountPath: /app
        env:
        - name: MODEL_REGISTRY_URL
          value: "http://model-registry:8000"
        - name: MINIO_ENDPOINT
          value: "minio:9000"
        - name: MINIO_ACCESS_KEY
          value: "admin"
        - name: MINIO_SECRET_KEY
          value: "minio123"
      volumes:
      - name: deployer-app
        configMap:
          name: model-deployer-app

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: model-deployer-app
  namespace: ml-serving
data:
  model_deployer.py: |
    #!/usr/bin/env python3
    import os
    import json
    import requests
    import joblib
    from datetime import datetime
    from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor
    from sklearn.linear_model import LogisticRegression
    from sklearn.datasets import make_classification, make_regression
    from minio import Minio
    import logging
    
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    
    # MinIO client
    minio_client = Minio(
        os.getenv("MINIO_ENDPOINT", "minio:9000"),
        access_key=os.getenv("MINIO_ACCESS_KEY", "admin"),
        secret_key=os.getenv("MINIO_SECRET_KEY", "minio123"),
        secure=False
    )
    
    bucket_name = "ml-models"
    
    def create_sample_models():
        """Create sample models for different architectures"""
        models = []
        
        # 1. Heavy model for AMD64
        logger.info("Creating heavy model for AMD64...")
        X, y = make_classification(n_samples=10000, n_features=100, n_classes=5, n_informative=80)
        heavy_model = RandomForestClassifier(n_estimators=200, max_depth=20)
        heavy_model.fit(X, y)
        
        models.append({
            "model": heavy_model,
            "name": "fraud_detection",
            "version": "v1.0",
            "framework": "sklearn",
            "architecture": "amd64",
            "description": "Heavy fraud detection model with 200 trees",
            "tags": ["classification", "fraud", "production"],
            "model_size_mb": 15.2,
            "performance_metrics": {
                "accuracy": 0.94,
                "precision": 0.92,
                "recall": 0.89,
                "f1_score": 0.905
            }
        })
        
        # 2. Lightweight model for ARM64
        logger.info("Creating lightweight model for ARM64...")
        X, y = make_classification(n_samples=5000, n_features=20, n_classes=2)
        light_model = LogisticRegression()
        light_model.fit(X, y)
        
        models.append({
            "model": light_model,
            "name": "spam_classifier",
            "version": "v2.1",
            "framework": "sklearn", 
            "architecture": "arm64",
            "description": "Lightweight spam classification model",
            "tags": ["classification", "spam", "lightweight"],
            "model_size_mb": 0.5,
            "performance_metrics": {
                "accuracy": 0.87,
                "precision": 0.85,
                "recall": 0.88,
                "f1_score": 0.865
            }
        })
        
        # 3. Edge model for ARM
        logger.info("Creating edge model for ARM...")
        X, y = make_regression(n_samples=1000, n_features=5, noise=0.1)
        edge_model = GradientBoostingRegressor(n_estimators=10, max_depth=3)
        edge_model.fit(X, y)
        
        models.append({
            "model": edge_model,
            "name": "sensor_predictor",
            "version": "v1.5",
            "framework": "edge-ml",
            "architecture": "arm",
            "description": "Ultra-lightweight sensor value predictor",
            "tags": ["regression", "iot", "edge"],
            "model_size_mb": 0.1,
            "performance_metrics": {
                "mse": 0.12,
                "mae": 0.08,
                "r2_score": 0.91
            }
        })
        
        return models
    
    def upload_model(model_info):
        """Upload model to MinIO and register in registry"""
        
        # Save model locally
        model_file = f"/tmp/{model_info['name']}_{model_info['version']}.joblib"
        metadata_file = f"/tmp/{model_info['name']}_{model_info['version']}_metadata.json"
        
        joblib.dump(model_info["model"], model_file)
        
        # Create metadata
        metadata = {k: v for k, v in model_info.items() if k != "model"}
        metadata["created_at"] = datetime.now().isoformat()
        
        with open(metadata_file, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        # Upload to MinIO
        model_path = f"models/{model_info['architecture']}/{model_info['name']}/{model_info['version']}.joblib"
        metadata_path = f"models/{model_info['architecture']}/{model_info['name']}/{model_info['version']}_metadata.json"
        
        try:
            minio_client.fput_object(bucket_name, model_path, model_file)
            minio_client.fput_object(bucket_name, metadata_path, metadata_file)
            logger.info(f"Uploaded model to {model_path}")
        except Exception as e:
            logger.error(f"Failed to upload model: {e}")
            return False
        
        # Register in model registry
        try:
            registry_url = os.getenv("MODEL_REGISTRY_URL", "http://model-registry:8000")
            response = requests.post(f"{registry_url}/models", json=metadata)
            
            if response.status_code == 200:
                logger.info(f"Registered model {model_info['name']}:{model_info['version']}")
                return True
            else:
                logger.error(f"Failed to register model: {response.text}")
                return False
                
        except Exception as e:
            logger.error(f"Failed to register model: {e}")
            return False
    
    def deploy_models():
        """Deploy all sample models"""
        models = create_sample_models()
        
        success_count = 0
        for model_info in models:
            if upload_model(model_info):
                success_count += 1
        
        logger.info(f"Successfully deployed {success_count}/{len(models)} models")
        
        # Test model registry
        try:
            registry_url = os.getenv("MODEL_REGISTRY_URL", "http://model-registry:8000")
            response = requests.get(f"{registry_url}/models")
            if response.status_code == 200:
                models = response.json()
                logger.info(f"Model registry contains {len(models)} models")
                for model in models:
                    logger.info(f"  - {model['name']}:{model['version']} ({model['architecture']})")
        except Exception as e:
            logger.error(f"Failed to query model registry: {e}")
    
    if __name__ == "__main__":
        deploy_models()

---
# Services
apiVersion: v1
kind: Service
metadata:
  name: minio
  namespace: ml-serving
spec:
  selector:
    app: minio
  ports:
  - name: api
    port: 9000
    targetPort: 9000
  - name: console
    port: 9001
    targetPort: 9001

---
apiVersion: v1
kind: Service
metadata:
  name: model-registry
  namespace: ml-serving
spec:
  selector:
    app: model-registry
  ports:
  - port: 8000
    targetPort: 8000

---
apiVersion: v1
kind: Service
metadata:
  name: tensorflow-serving
  namespace: ml-serving
spec:
  selector:
    app: tensorflow-serving
    architecture: amd64
  ports:
  - name: grpc
    port: 8500
    targetPort: 8500
  - name: http
    port: 8501
    targetPort: 8501

---
apiVersion: v1
kind: Service
metadata:
  name: sklearn-serving
  namespace: ml-serving
spec:
  selector:
    app: sklearn-serving
    architecture: arm64
  ports:
  - port: 8000
    targetPort: 8000

---
# Ingress
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ml-serving-ingress
  namespace: ml-serving
spec:
  rules:
  - host: ml.homelab.local
    http:
      paths:
      - path: /registry
        pathType: Prefix
        backend:
          service:
            name: model-registry
            port:
              number: 8000
      - path: /tensorflow
        pathType: Prefix
        backend:
          service:
            name: tensorflow-serving
            port:
              number: 8501
      - path: /sklearn
        pathType: Prefix
        backend:
          service:
            name: sklearn-serving
            port:
              number: 8000
  - host: minio.homelab.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: minio
            port:
              number: 9001

---
# RBAC
apiVersion: v1
kind: ServiceAccount
metadata:
  name: model-registry
  namespace: ml-serving

---
# PVC for models
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: tensorflow-models
  namespace: ml-serving
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: longhorn-ssd

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: sklearn-models
  namespace: ml-serving
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: longhorn-ssd

---
# TensorFlow configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: tensorflow-config
  namespace: ml-serving
data:
  model_config.txt: |
    model_config_list {
      config {
        name: 'my_model'
        base_path: '/mnt/models/my_model'
        model_platform: 'tensorflow'
        model_version_policy {
          latest {
            num_versions: 3
          }
        }
      }
    }
  
  monitoring_config.txt: |
    prometheus_config {
      enable: true,
      path: "/monitoring/prometheus/metrics"
    }

---
# Model performance monitoring
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-monitor
  namespace: ml-serving
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ml-monitor
  template:
    metadata:
      labels:
        app: ml-monitor
    spec:
      containers:
      - name: monitor
        image: python:3.11-slim
        command: ["/bin/bash"]
        args:
        - -c
        - |
          pip install requests prometheus-client numpy pandas
          python3 /app/ml_monitor.py
        volumeMounts:
        - name: monitor-app
          mountPath: /app
        env:
        - name: PROMETHEUS_PORT
          value: "8080"
        - name: SKLEARN_SERVICE
          value: "http://sklearn-serving:8000"
        - name: TENSORFLOW_SERVICE
          value: "http://tensorflow-serving:8501"
        ports:
        - containerPort: 8080
          name: metrics
        resources:
          requests:
            cpu: "50m"
            memory: "128Mi"
          limits:
            cpu: "200m"
            memory: "256Mi"
      volumes:
      - name: monitor-app
        configMap:
          name: ml-monitor-app

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ml-monitor-app
  namespace: ml-serving
data:
  ml_monitor.py: |
    #!/usr/bin/env python3
    import os
    import time
    import logging
    import requests
    import numpy as np
    from prometheus_client import start_http_server, Gauge, Counter, Histogram
    from typing import Dict
    
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    
    # Prometheus metrics
    MODEL_LATENCY = Histogram('ml_model_latency_seconds', 'Model inference latency', ['service', 'model', 'architecture'])
    MODEL_THROUGHPUT = Gauge('ml_model_throughput_qps', 'Model throughput', ['service', 'model', 'architecture'])
    MODEL_ERROR_RATE = Counter('ml_model_errors_total', 'Model errors', ['service', 'model', 'architecture'])
    MODEL_ACCURACY = Gauge('ml_model_accuracy', 'Model accuracy', ['service', 'model'])
    POWER_EFFICIENCY = Gauge('ml_power_efficiency_inferences_per_watt', 'Power efficiency', ['service', 'architecture'])
    
    class MLMonitor:
        def __init__(self):
            self.services = {
                "sklearn": {
                    "url": os.getenv("SKLEARN_SERVICE", "http://sklearn-serving:8000"),
                    "architecture": "arm64"
                },
                "tensorflow": {
                    "url": os.getenv("TENSORFLOW_SERVICE", "http://tensorflow-serving:8501"),
                    "architecture": "amd64"
                }
            }
            
            # Architecture power consumption (watts)
            self.arch_power = {
                "amd64": 45,
                "arm64": 7,
                "arm": 3
            }
        
        def test_sklearn_service(self) -> Dict:
            """Test sklearn service performance"""
            url = self.services["sklearn"]["url"]
            arch = self.services["sklearn"]["architecture"]
            
            try:
                # Test prediction
                test_data = np.random.randn(1, 20).tolist()
                payload = {"instances": test_data, "model_name": "default"}
                
                start_time = time.time()
                response = requests.post(f"{url}/predict", json=payload, timeout=5)
                latency = time.time() - start_time
                
                if response.status_code == 200:
                    result = response.json()
                    
                    MODEL_LATENCY.labels(service="sklearn", model="default", architecture=arch).observe(latency)
                    
                    # Calculate throughput with batch test
                    batch_response = requests.get(f"{url}/predict/batch/default?batch_size=32", timeout=10)
                    if batch_response.status_code == 200:
                        batch_result = batch_response.json()
                        throughput = batch_result.get("throughput_qps", 0)
                        MODEL_THROUGHPUT.labels(service="sklearn", model="default", architecture=arch).set(throughput)
                        
                        # Calculate power efficiency
                        power = self.arch_power.get(arch, 10)
                        efficiency = throughput / power
                        POWER_EFFICIENCY.labels(service="sklearn", architecture=arch).set(efficiency)
                    
                    return {"status": "ok", "latency": latency, "service": "sklearn"}
                else:
                    MODEL_ERROR_RATE.labels(service="sklearn", model="default", architecture=arch).inc()
                    return {"status": "error", "code": response.status_code}
                    
            except Exception as e:
                MODEL_ERROR_RATE.labels(service="sklearn", model="default", architecture=arch).inc()
                logger.error(f"sklearn test failed: {e}")
                return {"status": "error", "error": str(e)}
        
        def test_tensorflow_service(self) -> Dict:
            """Test TensorFlow service performance"""
            url = self.services["tensorflow"]["url"]
            arch = self.services["tensorflow"]["architecture"]
            
            try:
                # Check if models are loaded
                response = requests.get(f"{url}/v1/models", timeout=5)
                if response.status_code == 200:
                    models = response.json()
                    model_count = len(models.get("models", []))
                    MODEL_THROUGHPUT.labels(service="tensorflow", model="all", architecture=arch).set(model_count)
                    
                    return {"status": "ok", "models": model_count, "service": "tensorflow"}
                else:
                    return {"status": "error", "code": response.status_code}
                    
            except Exception as e:
                logger.error(f"tensorflow test failed: {e}")
                return {"status": "error", "error": str(e)}
        
        def run_monitoring(self):
            """Run continuous monitoring"""
            logger.info("Starting ML monitoring...")
            
            while True:
                try:
                    # Test sklearn service
                    sklearn_result = self.test_sklearn_service()
                    logger.info(f"Sklearn test: {sklearn_result}")
                    
                    # Test tensorflow service
                    tf_result = self.test_tensorflow_service()
                    logger.info(f"TensorFlow test: {tf_result}")
                    
                    # Sleep for 30 seconds
                    time.sleep(30)
                    
                except Exception as e:
                    logger.error(f"Monitoring error: {e}")
                    time.sleep(60)
    
    if __name__ == "__main__":
        # Start Prometheus metrics server
        prometheus_port = int(os.getenv("PROMETHEUS_PORT", "8080"))
        start_http_server(prometheus_port)
        logger.info(f"Prometheus metrics server started on port {prometheus_port}")
        
        # Start monitoring
        monitor = MLMonitor()
        monitor.run_monitoring()