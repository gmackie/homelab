# Prometheus alerting rules for homelab monitoring
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: homelab-alerts
  namespace: monitoring
  labels:
    prometheus: kube-prometheus
spec:
  groups:
  - name: homelab.rules
    interval: 30s
    rules:
    # Node alerts
    - alert: NodeDown
      expr: up{job="node-exporter"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Node {{ $labels.instance }} is down"
        description: "Node {{ $labels.instance }} has been down for more than 5 minutes"
    
    - alert: HighCPUUsage
      expr: (1 - avg(rate(node_cpu_seconds_total{mode="idle"}[5m]))) * 100 > 80
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "High CPU usage detected"
        description: "CPU usage is above 80% for more than 10 minutes"
    
    - alert: HighMemoryUsage
      expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "High memory usage detected"
        description: "Memory usage is above 85% for more than 10 minutes"
    
    # Storage alerts
    - alert: DiskSpaceLow
      expr: (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) * 100 < 15
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Low disk space on root filesystem"
        description: "Less than 15% disk space remaining"
    
    - alert: LonghornVolumeLow
      expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 > 80
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Longhorn volume {{ $labels.volume }} is filling up"
        description: "Volume is over 80% full"
    
    # Service alerts
    - alert: PodCrashLooping
      expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"
        description: "Pod has restarted {{ $value }} times in the last 15 minutes"
    
    - alert: PodNotReady
      expr: kube_pod_status_ready{condition="false"} == 1
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is not ready"
        description: "Pod has been not ready for more than 10 minutes"
    
    # Media stack alerts
    - alert: SonarrDown
      expr: up{job="exportarr-sonarr"} == 0
      for: 5m
      labels:
        severity: warning
        service: media
      annotations:
        summary: "Sonarr is down"
        description: "Sonarr has been unreachable for 5 minutes"
    
    - alert: RadarrDown
      expr: up{job="exportarr-radarr"} == 0
      for: 5m
      labels:
        severity: warning
        service: media
      annotations:
        summary: "Radarr is down"
        description: "Radarr has been unreachable for 5 minutes"
    
    - alert: SABnzbdQueueStalled
      expr: sabnzbd_queue_size > 0 and rate(sabnzbd_downloaded_bytes[5m]) == 0
      for: 30m
      labels:
        severity: warning
        service: media
      annotations:
        summary: "SABnzbd download queue is stalled"
        description: "Downloads in queue but no progress for 30 minutes"
    
    # Certificate alerts
    - alert: CertificateExpiringSoon
      expr: certmanager_certificate_expiration_timestamp_seconds - time() < 7 * 24 * 60 * 60
      for: 1h
      labels:
        severity: warning
      annotations:
        summary: "Certificate {{ $labels.name }} expiring soon"
        description: "Certificate will expire in less than 7 days"
    
    # Backup alerts
    - alert: BackupJobFailed
      expr: kube_job_failed{job_name=~"backup-.*"} > 0
      for: 1h
      labels:
        severity: warning
      annotations:
        summary: "Backup job {{ $labels.job_name }} failed"
        description: "Backup job has failed status"
---
# AlertManager configuration
apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-config
  namespace: monitoring
stringData:
  alertmanager.yaml: |
    global:
      resolve_timeout: 5m
    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 12h
      receiver: 'default'
      routes:
      - match:
          severity: critical
        receiver: 'critical'
    receivers:
    - name: 'default'
      webhook_configs:
      - url: 'http://localhost:5001/'  # Replace with your webhook
    - name: 'critical'
      webhook_configs:
      - url: 'http://localhost:5001/critical'  # Replace with your critical webhook
      # email_configs:
      # - to: 'your-email@example.com'
      #   from: 'alertmanager@mackie.house'
      #   smarthost: 'smtp.gmail.com:587'
      #   auth_username: 'your-email@gmail.com'
      #   auth_password: 'your-app-password'